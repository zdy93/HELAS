{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and explore data\n",
    "\n",
    "We'll start off with reading and doing some basic data processing. We'll assume that:\n",
    "* you've downloaded the data from http://www.eraserbenchmark.com/ and have unpacked it to a directory called `data`\n",
    "* you're running the kernel in the root of the `eraserbenchmark` repo\n",
    "\n",
    "We're going to work with the movies dataset as it's the smallest and easiest to get started with. All the data is stored in either plain text, or jsonl, and should be pre-tokenized and ready to go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from rationale_benchmark.utils import load_documents, load_datasets, annotations_from_jsonl, Annotation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_root = os.path.join('data', 'movies')\n",
    "data_root = \"/work/dzhang5/attention/movie_data\"\n",
    "documents = load_documents(data_root)\n",
    "val = annotations_from_jsonl(os.path.join(data_root, 'val.jsonl'))\n",
    "## Or load everything:\n",
    "train, val, test = load_datasets(data_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'rationale_benchmark.utils.Annotation'>\n",
      "What is the sentiment of this review?\n",
      "NEG\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "ann = train[0]\n",
    "evidences = ann.all_evidences()\n",
    "print(type(ann))\n",
    "print(ann.query)\n",
    "print(ann.classification)\n",
    "print(len(evidences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract text, label and attention label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_attention_label(attention_label, evidences):\n",
    "    starts = [ev.start_token for ev in evidences]\n",
    "    ends = [ev.end_token for ev in evidences]\n",
    "    att_len = attention_label.shape[0]\n",
    "    for s, e in zip(starts, ends):\n",
    "        assert 0 <= s <=e <=att_len\n",
    "        attention_label[s:e] = 1\n",
    "    return attention_label\n",
    "\n",
    "def get_list_data(data, documents):\n",
    "    attention_label_list = []\n",
    "    y_list = []\n",
    "    x_list = []\n",
    "    for e, ann in enumerate(data):\n",
    "        evidences = ann.all_evidences()\n",
    "        assert ann.classification in ['NEG', 'POS']\n",
    "        if ann.classification == \"NEG\":\n",
    "            y_list.append(0)\n",
    "        elif ann.classification == 'POS':\n",
    "            y_list.append(1)\n",
    "        docid = ann.annotation_id\n",
    "        doc = documents[docid]\n",
    "        flattened_doc = list(itertools.chain.from_iterable(doc))\n",
    "        x_list.append(flattened_doc)\n",
    "        doc_len = len(flattened_doc)\n",
    "        attention_label = np.zeros(doc_len)\n",
    "        try:\n",
    "            attention_label = update_attention_label(attention_label, evidences)\n",
    "            attention_label_list.append(attention_label)\n",
    "        except AssertionError:\n",
    "            print(f\"id is: {e}, docid is: {docid}\")\n",
    "    return x_list, y_list, attention_label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "import six\n",
    "\n",
    "def pad_sequences(sequences, maxlen=None, dtype='int32',\n",
    "                  padding='pre', truncating='pre', value=0.):\n",
    "    \"\"\"Pads sequences to the same length.\n",
    "\n",
    "    This function transforms a list of\n",
    "    `num_samples` sequences (lists of integers)\n",
    "    into a 2D Numpy array of shape `(num_samples, num_timesteps)`.\n",
    "    `num_timesteps` is either the `maxlen` argument if provided,\n",
    "    or the length of the longest sequence otherwise.\n",
    "\n",
    "    Sequences that are shorter than `num_timesteps`\n",
    "    are padded with `value` at the end.\n",
    "\n",
    "    Sequences longer than `num_timesteps` are truncated\n",
    "    so that they fit the desired length.\n",
    "    The position where padding or truncation happens is determined by\n",
    "    the arguments `padding` and `truncating`, respectively.\n",
    "\n",
    "    Pre-padding is the default.\n",
    "\n",
    "    # Arguments\n",
    "        sequences: List of lists, where each element is a sequence.\n",
    "        maxlen: Int, maximum length of all sequences.\n",
    "        dtype: Type of the output sequences.\n",
    "            To pad sequences with variable length strings, you can use `object`.\n",
    "        padding: String, 'pre' or 'post':\n",
    "            pad either before or after each sequence.\n",
    "        truncating: String, 'pre' or 'post':\n",
    "            remove values from sequences larger than\n",
    "            `maxlen`, either at the beginning or at the end of the sequences.\n",
    "        value: Float or String, padding value.\n",
    "\n",
    "    # Returns\n",
    "        x: Numpy array with shape `(len(sequences), maxlen)`\n",
    "\n",
    "    # Raises\n",
    "        ValueError: In case of invalid values for `truncating` or `padding`,\n",
    "            or in case of invalid shape for a `sequences` entry.\n",
    "    \"\"\"\n",
    "    if not hasattr(sequences, '__len__'):\n",
    "        raise ValueError('`sequences` must be iterable.')\n",
    "    num_samples = len(sequences)\n",
    "\n",
    "    lengths = []\n",
    "    for x in sequences:\n",
    "        try:\n",
    "            lengths.append(len(x))\n",
    "        except TypeError:\n",
    "            raise ValueError('`sequences` must be a list of iterables. '\n",
    "                             'Found non-iterable: ' + str(x))\n",
    "\n",
    "    if maxlen is None:\n",
    "        maxlen = np.max(lengths)\n",
    "\n",
    "    # take the sample shape from the first non empty sequence\n",
    "    # checking for consistency in the main loop below.\n",
    "    sample_shape = tuple()\n",
    "    for s in sequences:\n",
    "        if len(s) > 0:\n",
    "            sample_shape = np.asarray(s).shape[1:]\n",
    "            break\n",
    "\n",
    "    is_dtype_str = np.issubdtype(dtype, np.str_) or np.issubdtype(dtype, np.unicode_)\n",
    "    if isinstance(value, six.string_types) and dtype != object and not is_dtype_str:\n",
    "        raise ValueError(\"`dtype` {} is not compatible with `value`'s type: {}\\n\"\n",
    "                         \"You should set `dtype=object` for variable length strings.\"\n",
    "                         .format(dtype, type(value)))\n",
    "\n",
    "    x = np.full((num_samples, maxlen) + sample_shape, value, dtype=dtype)\n",
    "    for idx, s in enumerate(sequences):\n",
    "        if not len(s):\n",
    "            continue  # empty list/array was found\n",
    "        if truncating == 'pre':\n",
    "            trunc = s[-maxlen:]\n",
    "        elif truncating == 'post':\n",
    "            trunc = s[:maxlen]\n",
    "        else:\n",
    "            raise ValueError('Truncating type \"%s\" '\n",
    "                             'not understood' % truncating)\n",
    "\n",
    "        # check `trunc` has expected shape\n",
    "        trunc = np.asarray(trunc, dtype=dtype)\n",
    "        if trunc.shape[1:] != sample_shape:\n",
    "            raise ValueError('Shape of sample %s of sequence at position %s '\n",
    "                             'is different from expected shape %s' %\n",
    "                             (trunc.shape[1:], idx, sample_shape))\n",
    "\n",
    "        if padding == 'post':\n",
    "            x[idx, :len(trunc)] = trunc\n",
    "        elif padding == 'pre':\n",
    "            x[idx, -len(trunc):] = trunc\n",
    "        else:\n",
    "            raise ValueError('Padding type \"%s\" not understood' % padding)\n",
    "    return x\n",
    "\n",
    "def get_list_data_with_head_attention(data, documents):\n",
    "    attention_label_list = []\n",
    "    y_list = []\n",
    "    x_list = []\n",
    "    id_list = []\n",
    "    for e, ann in enumerate(data):\n",
    "        evidences = ann.all_evidences()\n",
    "        assert ann.classification in ['NEG', 'POS']\n",
    "        docid = ann.annotation_id\n",
    "        doc = documents[docid]\n",
    "        flattened_doc = list(itertools.chain.from_iterable(doc))\n",
    "        doc_len = len(flattened_doc)\n",
    "        attention_label = np.zeros(doc_len)\n",
    "        try:\n",
    "            attention_label = update_attention_label(attention_label, evidences)\n",
    "        except AssertionError:\n",
    "            print(f\"id is: {e}, docid is: {docid}\")\n",
    "        if attention_label[0:200].sum() > 0:\n",
    "            id_list.append(docid)\n",
    "            x_list.append(flattened_doc[0:200])\n",
    "            attention_label_list.append(attention_label[0:200])\n",
    "            if ann.classification == \"NEG\":\n",
    "                y_list.append(0)\n",
    "            elif ann.classification == 'POS':\n",
    "                y_list.append(1)\n",
    "    attention_label_list = pad_sequences(attention_label_list, maxlen=200, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "    return x_list, y_list, attention_label_list, id_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, attention_label_train = get_list_data(train, documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_val, y_val, attention_label_val = get_list_data(val, documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test, y_test, attention_label_test = get_list_data(test, documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, attention_label_train, id_train = get_list_data_with_head_attention(train, documents)\n",
    "x_val, y_val, attention_label_val, id_val = get_list_data_with_head_attention(val, documents)\n",
    "x_test, y_test, attention_label_test, id_test = get_list_data_with_head_attention(test, documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(170, 200)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_label_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_val_test = x_val.copy()\n",
    "x_val_test.extend(x_test)\n",
    "y_val_test = y_val.copy()\n",
    "y_val_test.extend(y_test)\n",
    "attention_label_val_test = np.concatenate([attention_label_val, attention_label_test])\n",
    "id_val_test = id_val.copy()\n",
    "id_val_test.extend(id_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array(y_train)\n",
    "y_val_test = np.array(y_val_test)\n",
    "y_val = np.array(y_val)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1241 150 170\n"
     ]
    }
   ],
   "source": [
    "print(len(y_train), len(y_val), len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-d89b08a0ade8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./movie_data/raw_text_train.npy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./movie_data/raw_text_val_test.npy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_val_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./movie_data/raw_text_val.npy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./movie_data/raw_text_test.npy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'x_train' is not defined"
     ]
    }
   ],
   "source": [
    "np.save(\"./movie_data/raw_text_train.npy\", x_train)\n",
    "np.save(\"./movie_data/raw_text_val_test.npy\", x_val_test)\n",
    "np.save(\"./movie_data/raw_text_val.npy\", x_val)\n",
    "np.save(\"./movie_data/raw_text_test.npy\", x_test)\n",
    "\n",
    "np.save(\"./movie_data/y_train.npy\", y_train)\n",
    "np.save(\"./movie_data/y_val_test.npy\", y_val_test)\n",
    "np.save(\"./movie_data/y_val.npy\", y_val)\n",
    "np.save(\"./movie_data/y_test.npy\", y_test)\n",
    "\n",
    "np.save(\"./movie_data/id_train.npy\", id_train)\n",
    "np.save(\"./movie_data/id_test.npy\", id_val_test)\n",
    "np.save(\"./movie_data/id_val.npy\", id_val)\n",
    "np.save(\"./movie_data/movie_data/id_test.npy\", id_test)\n",
    "\n",
    "np.save(\"./movie_data/att_labels_train.npy\", attention_label_train)\n",
    "np.save(\"./movie_data/att_labels_val_test.npy\", attention_label_val_test)\n",
    "np.save(\"./movie_data/att_labels_val.npy\", attention_label_val)\n",
    "np.save(\"../movie_data/attention/movie_data/att_labels_test.npy\", attention_label_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check length after tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_align(orig_tokens, orig_attentions, tokenizer):\n",
    "    \"\"\"\n",
    "    tokenize a sentence and generate corresponding attention labels\n",
    "    \"\"\"\n",
    "    if type(orig_tokens) is list:\n",
    "        orig_tokens = orig_tokens[0:200]\n",
    "    else:\n",
    "        orig_tokens = orig_tokens.split()\n",
    "    bert_tokens = []\n",
    "    new_attentions = []\n",
    "    bert_tokens.append(\"[CLS]\")\n",
    "    new_attentions.append(0)\n",
    "    for orig_token, orig_attent in zip(orig_tokens, orig_attentions):\n",
    "        token = tokenizer.tokenize(orig_token)\n",
    "        bert_tokens.extend(token)\n",
    "        new_attentions.extend([orig_attent for i in token])\n",
    "    bert_tokens.append(\"[SEP]\")\n",
    "    new_attentions.append(0)\n",
    "    return bert_tokens, new_attentions\n",
    "\n",
    "\n",
    "def token_align_float(orig_tokens, orig_attentions, tokenizer):\n",
    "    \"\"\"\n",
    "    tokenize a sentence and generate corresponding attention labels (float)\n",
    "    \"\"\"\n",
    "    if type(orig_tokens) is list:\n",
    "        orig_tokens = orig_tokens[0:200]\n",
    "    else:\n",
    "        orig_tokens = orig_tokens.split()\n",
    "    bert_tokens = []\n",
    "    new_attentions = []\n",
    "    bert_tokens.append(\"[CLS]\")\n",
    "    new_attentions.append(0.0)\n",
    "    for orig_token, orig_attent in zip(orig_tokens, orig_attentions):\n",
    "        token = tokenizer.tokenize(orig_token)\n",
    "        bert_tokens.extend(token)\n",
    "        new_attentions.extend([orig_attent for i in token])\n",
    "    bert_tokens.append(\"[SEP]\")\n",
    "    new_attentions.append(0.0)\n",
    "    return bert_tokens, new_attentions\n",
    "\n",
    "\n",
    "def token_align_two(orig_tokens, orig_attentions, orig_attentions_for_val, tokenizer):\n",
    "    \"\"\"\n",
    "    tokenize a sentence and generate two corresponding attention labels\n",
    "    \"\"\"\n",
    "    if type(orig_tokens) is list:\n",
    "        orig_tokens = orig_tokens[0:200]\n",
    "    else:\n",
    "        orig_tokens = orig_tokens.split()\n",
    "    bert_tokens = []\n",
    "    new_attentions = []\n",
    "    new_attentions_val = []\n",
    "    bert_tokens.append(\"[CLS]\")\n",
    "    new_attentions.append(0)\n",
    "    new_attentions_val.append(0)\n",
    "    for orig_token, orig_attent, orig_attent_val in zip(orig_tokens, orig_attentions, orig_attentions_for_val):\n",
    "        token = tokenizer.tokenize(orig_token)\n",
    "        bert_tokens.extend(token)\n",
    "        new_attentions.extend([orig_attent for i in token])\n",
    "        new_attentions_val.extend([orig_attent_val for i in token])\n",
    "    bert_tokens.append(\"[SEP]\")\n",
    "    new_attentions.append(0)\n",
    "    new_attentions_val.append(0)\n",
    "    return bert_tokens, new_attentions, new_attentions_val\n",
    "\n",
    "\n",
    "def tokenize_with_new_attentions(orig_text, orig_attention_list, max_length, tokenizer, if_float=False):\n",
    "    \"\"\"\n",
    "    tokenize a array of raw text and generate corresponding\n",
    "    attention labels array and attention masks array\n",
    "    \"\"\"\n",
    "    if if_float == True:\n",
    "        tokens_attents = [token_align_float(r, a, tokenizer) for r, a in zip(orig_text, orig_attention_list)]\n",
    "    else:\n",
    "        tokens_attents = [token_align(r, a, tokenizer) for r, a in zip(orig_text, orig_attention_list)]\n",
    "    bert_tokens = [i[0] for i in tokens_attents]\n",
    "    attent_labels = [i[1] for i in tokens_attents]\n",
    "    input_ids = [tokenizer.convert_tokens_to_ids(x) for x in bert_tokens]\n",
    "    input_ids = pad_sequences(input_ids, maxlen=max_length, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "    if if_float == True:\n",
    "        attent_labels = pad_sequences(attent_labels, maxlen=max_length, dtype=\"float\", truncating=\"post\",\n",
    "                                      padding=\"post\")\n",
    "    else:\n",
    "        attent_labels = pad_sequences(attent_labels, maxlen=max_length, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "    attention_masks = []\n",
    "    for seq in input_ids:\n",
    "        seq_mask = [float(i > 0) for i in seq]\n",
    "        attention_masks.append(seq_mask)\n",
    "    attention_masks = np.array(attention_masks)\n",
    "    return input_ids, attent_labels, attention_masks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_token_train, att_token_train = [], []\n",
    "for orig_x, orig_att in zip(x_train, attention_label_train):\n",
    "    bert_x, bert_att = token_align(orig_x, orig_att, tokenizer)\n",
    "    x_token_train.append(bert_x)\n",
    "    att_token_train.append(bert_att)\n",
    "\n",
    "x_token_val, att_token_val = [], []\n",
    "for orig_x, orig_att in zip(x_val, attention_label_val):\n",
    "    bert_x, bert_att = token_align(orig_x, orig_att, tokenizer)\n",
    "    x_token_val.append(bert_x)\n",
    "    att_token_val.append(bert_att)\n",
    "    \n",
    "x_token_test, att_token_test = [], []\n",
    "for orig_x, orig_att in zip(x_test, attention_label_test):\n",
    "    bert_x, bert_att = token_align(orig_x, orig_att, tokenizer)\n",
    "    x_token_test.append(bert_x)\n",
    "    att_token_test.append(bert_att)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len_train = max([len(bert_x) for bert_x in x_token_train])\n",
    "max_len_val = max([len(bert_x) for bert_x in x_token_val])\n",
    "max_len_test = max([len(bert_x) for bert_x in x_token_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "264 267 258\n"
     ]
    }
   ],
   "source": [
    "print(max_len_train, max_len_val, max_len_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "220.0"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.median([len(bert_x) for bert_x in x_token_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After tokenize,we could choose 256 as max length for movie data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
