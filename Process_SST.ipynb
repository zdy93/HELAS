{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process SST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xlrd\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No config specified, defaulting to: sst/default\n",
      "Reusing dataset sst (/home/dzhang5/.cache/huggingface/datasets/sst/default/1.0.0/b8a7889ef01c5d3ae8c379b84cc4080f8aad3ac2bc538701cbe0ac6416fb76ff)\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset('sst')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence', 'label', 'tokens', 'tree'],\n",
       "        num_rows: 8544\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence', 'label', 'tokens', 'tree'],\n",
       "        num_rows: 1101\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence', 'label', 'tokens', 'tree'],\n",
       "        num_rows: 2210\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['train'].set_format(type='pandas')\n",
    "dataset['validation'].set_format(type='pandas')\n",
    "dataset['test'].set_format(type='pandas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = dataset['train'][:]\n",
    "val_data = dataset['validation'][:]\n",
    "test_data = dataset['test'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>tokens</th>\n",
       "      <th>tree</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Rock is destined to be the 21st Century 's...</td>\n",
       "      <td>0.69444</td>\n",
       "      <td>The|Rock|is|destined|to|be|the|21st|Century|'s...</td>\n",
       "      <td>70|70|68|67|63|62|61|60|58|58|57|56|56|64|65|5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The gorgeously elaborate continuation of `` Th...</td>\n",
       "      <td>0.83333</td>\n",
       "      <td>The|gorgeously|elaborate|continuation|of|``|Th...</td>\n",
       "      <td>71|70|69|69|67|67|66|64|63|62|62|61|61|58|57|5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Singer\\/composer Bryan Adams contributes a sle...</td>\n",
       "      <td>0.62500</td>\n",
       "      <td>Singer\\/composer|Bryan|Adams|contributes|a|sle...</td>\n",
       "      <td>72|71|71|70|68|68|67|67|66|63|62|62|60|60|58|5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>You 'd think by now America would have had eno...</td>\n",
       "      <td>0.50000</td>\n",
       "      <td>You|'d|think|by|now|America|would|have|had|eno...</td>\n",
       "      <td>36|35|34|33|33|32|30|29|27|26|25|24|23|23|22|2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Yet the act is still charming here .</td>\n",
       "      <td>0.72222</td>\n",
       "      <td>Yet|the|act|is|still|charming|here|.</td>\n",
       "      <td>15|13|13|10|9|9|11|12|10|11|12|14|14|15|0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence    label  \\\n",
       "0  The Rock is destined to be the 21st Century 's...  0.69444   \n",
       "1  The gorgeously elaborate continuation of `` Th...  0.83333   \n",
       "2  Singer\\/composer Bryan Adams contributes a sle...  0.62500   \n",
       "3  You 'd think by now America would have had eno...  0.50000   \n",
       "4               Yet the act is still charming here .  0.72222   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  The|Rock|is|destined|to|be|the|21st|Century|'s...   \n",
       "1  The|gorgeously|elaborate|continuation|of|``|Th...   \n",
       "2  Singer\\/composer|Bryan|Adams|contributes|a|sle...   \n",
       "3  You|'d|think|by|now|America|would|have|had|eno...   \n",
       "4               Yet|the|act|is|still|charming|here|.   \n",
       "\n",
       "                                                tree  \n",
       "0  70|70|68|67|63|62|61|60|58|58|57|56|56|64|65|5...  \n",
       "1  71|70|69|69|67|67|66|64|63|62|62|61|61|58|57|5...  \n",
       "2  72|71|71|70|68|68|67|67|66|63|62|62|60|60|58|5...  \n",
       "3  36|35|34|33|33|32|30|29|27|26|25|24|23|23|22|2...  \n",
       "4          15|13|13|10|9|9|11|12|10|11|12|14|14|15|0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['token_list'] = train_data['tokens'].apply(lambda x: x.split('|'))\n",
    "val_data['token_list'] = val_data['tokens'].apply(lambda x: x.split('|'))\n",
    "test_data['token_list'] = test_data['tokens'].apply(lambda x: x.split('|'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['binary_label'] = train_data['label'] >= 0.5\n",
    "val_data['binary_label'] = val_data['label'] >= 0.5\n",
    "test_data['binary_label'] = test_data['label'] >= 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['token_list_len'] = train_data['token_list'].apply(len)\n",
    "val_data['token_list_len'] = val_data['token_list'].apply(len)\n",
    "test_data['token_list_len'] = test_data['token_list'].apply(len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract data for human annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7160"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(train_data['token_list_len'] >=10).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1624"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((train_data['label'] <=0.6)&(train_data['label'] >=0.4)).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos_candidate = train_data.loc[(train_data['label'] >=0.6)&(train_data['token_list_len'] >=15)]\n",
    "train_neg_candidate = train_data.loc[(train_data['label'] <0.4)&(train_data['token_list_len'] >=15)]\n",
    "\n",
    "val_pos_candidate = val_data.loc[(val_data['label'] >=0.6)&(val_data['token_list_len'] >=15)]\n",
    "val_neg_candidate = val_data.loc[(val_data['label'] <0.4)&(val_data['token_list_len'] >=15)]\n",
    "\n",
    "test_pos_candidate = test_data.loc[((test_data['label'] >=0.6))&(test_data['token_list_len'] >=15)]\n",
    "test_neg_candidate = test_data.loc[(test_data['label'] <0.4)&(test_data['token_list_len'] >=15)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 1\n",
    "_, train_pos_ham_index = train_test_split(train_pos_candidate.index, test_size=100, random_state=random_state)\n",
    "_, train_neg_ham_index = train_test_split(train_neg_candidate.index, test_size=100, random_state=random_state)\n",
    "\n",
    "_, val_pos_ham_index = train_test_split(val_pos_candidate.index, test_size=50, random_state=random_state)\n",
    "_, val_neg_ham_index = train_test_split(val_neg_candidate.index, test_size=50, random_state=random_state)\n",
    "\n",
    "_, test_pos_ham_index = train_test_split(test_pos_candidate.index, test_size=50, random_state=random_state)\n",
    "_, test_neg_ham_index = train_test_split(test_neg_candidate.index, test_size=50, random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos = train_pos_candidate.loc[train_pos_ham_index]\n",
    "train_neg = train_neg_candidate.loc[train_neg_ham_index]\n",
    "\n",
    "val_pos = val_pos_candidate.loc[val_pos_ham_index]\n",
    "val_neg = val_neg_candidate.loc[val_neg_ham_index]\n",
    "\n",
    "test_pos = test_pos_candidate.loc[test_pos_ham_index]\n",
    "test_neg = test_neg_candidate.loc[test_neg_ham_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_all = pd.concat([val_pos, val_neg])\n",
    "test_all = pd.concat([test_pos, test_neg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>tokens</th>\n",
       "      <th>tree</th>\n",
       "      <th>token_list</th>\n",
       "      <th>token_list_len</th>\n",
       "      <th>binary_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2205</th>\n",
       "      <td>An imaginative comedy\\/thriller .</td>\n",
       "      <td>0.777780</td>\n",
       "      <td>An|imaginative|comedy\\/thriller|.</td>\n",
       "      <td>7|6|5|5|6|7|0</td>\n",
       "      <td>[An, imaginative, comedy\\/thriller, .]</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2206</th>\n",
       "      <td>( A ) rare , beautiful film .</td>\n",
       "      <td>0.916670</td>\n",
       "      <td>(|A|)|rare|,|beautiful|film|.</td>\n",
       "      <td>13|12|12|11|10|9|9|15|10|11|14|13|14|15|0</td>\n",
       "      <td>[(, A, ), rare, ,, beautiful, film, .]</td>\n",
       "      <td>8</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2207</th>\n",
       "      <td>( An ) hilarious romantic comedy .</td>\n",
       "      <td>0.888890</td>\n",
       "      <td>(|An|)|hilarious|romantic|comedy|.</td>\n",
       "      <td>12|11|11|9|8|8|10|9|10|13|12|13|0</td>\n",
       "      <td>[(, An, ), hilarious, romantic, comedy, .]</td>\n",
       "      <td>7</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2208</th>\n",
       "      <td>Never ( sinks ) into exploitation .</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>Never|(|sinks|)|into|exploitation|.</td>\n",
       "      <td>11|10|9|9|8|8|13|12|10|11|12|13|0</td>\n",
       "      <td>[Never, (, sinks, ), into, exploitation, .]</td>\n",
       "      <td>7</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2209</th>\n",
       "      <td>( U ) nrelentingly stupid .</td>\n",
       "      <td>0.069444</td>\n",
       "      <td>(|U|)|nrelentingly|stupid|.</td>\n",
       "      <td>10|9|9|7|7|8|8|11|10|11|0</td>\n",
       "      <td>[(, U, ), nrelentingly, stupid, .]</td>\n",
       "      <td>6</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 sentence     label  \\\n",
       "2205    An imaginative comedy\\/thriller .  0.777780   \n",
       "2206        ( A ) rare , beautiful film .  0.916670   \n",
       "2207   ( An ) hilarious romantic comedy .  0.888890   \n",
       "2208  Never ( sinks ) into exploitation .  0.625000   \n",
       "2209          ( U ) nrelentingly stupid .  0.069444   \n",
       "\n",
       "                                   tokens  \\\n",
       "2205    An|imaginative|comedy\\/thriller|.   \n",
       "2206        (|A|)|rare|,|beautiful|film|.   \n",
       "2207   (|An|)|hilarious|romantic|comedy|.   \n",
       "2208  Never|(|sinks|)|into|exploitation|.   \n",
       "2209          (|U|)|nrelentingly|stupid|.   \n",
       "\n",
       "                                           tree  \\\n",
       "2205                              7|6|5|5|6|7|0   \n",
       "2206  13|12|12|11|10|9|9|15|10|11|14|13|14|15|0   \n",
       "2207          12|11|11|9|8|8|10|9|10|13|12|13|0   \n",
       "2208          11|10|9|9|8|8|13|12|10|11|12|13|0   \n",
       "2209                  10|9|9|7|7|8|8|11|10|11|0   \n",
       "\n",
       "                                       token_list  token_list_len  \\\n",
       "2205       [An, imaginative, comedy\\/thriller, .]               4   \n",
       "2206       [(, A, ), rare, ,, beautiful, film, .]               8   \n",
       "2207   [(, An, ), hilarious, romantic, comedy, .]               7   \n",
       "2208  [Never, (, sinks, ), into, exploitation, .]               7   \n",
       "2209           [(, U, ), nrelentingly, stupid, .]               6   \n",
       "\n",
       "      binary_label  \n",
       "2205          True  \n",
       "2206          True  \n",
       "2207          True  \n",
       "2208          True  \n",
       "2209         False  "
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_xlsx(data, xname):\n",
    "    max_len = data['token_list_len'].max()\n",
    "    with xlsxwriter.Workbook(xname) as workbook:\n",
    "        worksheet = workbook.add_worksheet()\n",
    "        line_ix = 0\n",
    "        first_line = ['sentence_index', 'binary_label', 'float_label'] + [f'token_{i}' for i in range(max_len)]\n",
    "        worksheet.write_row(line_ix, 0, first_line)\n",
    "        line_ix += 1\n",
    "\n",
    "        for i in range(data.shape[0]):\n",
    "            line_1 = [int(data.index[i]), int(data['binary_label'].iloc[i]), data['label'].iloc[i]] + data['token_list'].iloc[i]\n",
    "            line_2 = ['', '', ''] + [0 for w in data['token_list'].iloc[i]]\n",
    "            line_3 = ['']\n",
    "            worksheet.write_row(line_ix, 0, line_1)\n",
    "            line_ix += 1\n",
    "            worksheet.write_row(line_ix, 0, line_2)\n",
    "            line_ix += 1\n",
    "            worksheet.write_row(line_ix, 0, line_3)\n",
    "            line_ix += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_to_xlsx(train_pos, 'senti_data/train_pos.xlsx')\n",
    "write_to_xlsx(train_neg, 'senti_data/train_neg.xlsx')\n",
    "write_to_xlsx(val_all, 'senti_data/val_mix.xlsx')\n",
    "write_to_xlsx(test_all, 'senti_data/test_mix.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load saved data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_pickle('./senti_data/train.p')\n",
    "val_data = pd.read_pickle('./senti_data/val.p')\n",
    "test_data = pd.read_pickle('./senti_data/test.p')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load labeled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tid_list, b_label_list, text_list, att_list = [], [], [], []\n",
    "workbook = xlrd.open_workbook('senti_data/train_pos_labeled.xlsx')\n",
    "for sheet in workbook.sheets():\n",
    "    for row in range(sheet.nrows):\n",
    "        if row == 0:\n",
    "            pass\n",
    "        if (row-1) % 3 == 0:\n",
    "            tid = sheet.cell(row, 0).value\n",
    "            b_label = sheet.cell(row, 1).value\n",
    "            text = [i.value for i in sheet.row(row)[3:] if i.ctype != 0]\n",
    "            tid_list.append(int(tid))\n",
    "            b_label_list.append(int(b_label))\n",
    "            text_list.append(text)\n",
    "        if (row-1) %3 == 1:\n",
    "            att = [int(i.value) for i in sheet.row(row)[3:] if i.ctype != 0]\n",
    "            if any([a not in [0, 1] for a in att]):\n",
    "                print(row)\n",
    "            att_list.append(att)\n",
    "            \n",
    "workbook = xlrd.open_workbook('senti_data/train_neg_labeled.xlsx')\n",
    "for sheet in workbook.sheets():\n",
    "    for row in range(sheet.nrows):\n",
    "        if row == 0:\n",
    "            pass\n",
    "        if (row-1) % 3 == 0:\n",
    "            tid = sheet.cell(row, 0).value\n",
    "            b_label = sheet.cell(row, 1).value\n",
    "            text = [i.value for i in sheet.row(row)[3:] if i.ctype != 0]\n",
    "            tid_list.append(int(tid))\n",
    "            b_label_list.append(int(b_label))\n",
    "            text_list.append(text)\n",
    "        if (row-1) %3 == 1:\n",
    "            att = [int(i.value) for i in sheet.row(row)[3:] if i.ctype != 0]\n",
    "            if any([a not in [0, 1] for a in att]):\n",
    "                print(row)\n",
    "            att_list.append(att)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "vtid_list, vb_label_list, vtext_list, vatt_list = [], [], [], []\n",
    "workbook = xlrd.open_workbook('senti_data/val_mix_labeled.xlsx')\n",
    "for sheet in workbook.sheets():\n",
    "    for row in range(sheet.nrows):\n",
    "        if row == 0:\n",
    "            pass\n",
    "        if (row-1) % 3 == 0:\n",
    "            tid = sheet.cell(row, 0).value\n",
    "            b_label = sheet.cell(row, 1).value\n",
    "            text = [i.value for i in sheet.row(row)[3:] if i.ctype != 0]\n",
    "            vtid_list.append(int(tid))\n",
    "            vb_label_list.append(int(b_label))\n",
    "            vtext_list.append(text)\n",
    "        if (row-1) %3 == 1:\n",
    "            att = [int(i.value) for i in sheet.row(row)[3:] if i.ctype != 0]\n",
    "            if any([a not in [0, 1] for a in att]):\n",
    "                print(row)\n",
    "            vatt_list.append(att)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttid_list, tb_label_list, ttext_list, tatt_list = [], [], [], []\n",
    "workbook = xlrd.open_workbook('senti_data/test_mix_labeled.xlsx')\n",
    "for sheet in workbook.sheets():\n",
    "    for row in range(sheet.nrows):\n",
    "        if row == 0:\n",
    "            pass\n",
    "        if (row-1) % 3 == 0:\n",
    "            tid = sheet.cell(row, 0).value\n",
    "            b_label = sheet.cell(row, 1).value\n",
    "            text = [i.value for i in sheet.row(row)[3:] if i.ctype != 0]\n",
    "            ttid_list.append(int(tid))\n",
    "            tb_label_list.append(int(b_label))\n",
    "            ttext_list.append(text)\n",
    "        if (row-1) %3 == 1:\n",
    "            att = [int(i.value) for i in sheet.row(row)[3:] if i.ctype != 0]\n",
    "            if any([a not in [0, 1] for a in att]):\n",
    "                print(row)\n",
    "            tatt_list.append(att)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_without_att = train_data.loc[~train_data.index.isin(tid_list)]\n",
    "train_data_with_att = train_data.loc[train_data.index.isin(tid_list)]\n",
    "val_data_without_att = val_data.loc[~val_data.index.isin(vtid_list)]\n",
    "val_data_with_att = val_data.loc[val_data.index.isin(vtid_list)]\n",
    "test_data_without_att = test_data.loc[~test_data.index.isin(ttid_list)]\n",
    "test_data_with_att = test_data.loc[test_data.index.isin(ttid_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_att_data = pd.DataFrame({'attention':att_list}, index=tid_list)\n",
    "train_data_with_att = train_data_with_att.merge(train_att_data, left_index=True, right_index=True, validate='1:1')\n",
    "\n",
    "val_att_data = pd.DataFrame({'attention':vatt_list}, index=vtid_list)\n",
    "val_data_with_att = val_data_with_att.merge(val_att_data, left_index=True, right_index=True, validate='1:1')\n",
    "\n",
    "test_att_data = pd.DataFrame({'attention':tatt_list}, index=ttid_list)\n",
    "test_data_with_att = test_data_with_att.merge(test_att_data, left_index=True, right_index=True, validate='1:1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "embeddings_index = dict()\n",
    "with open('glove.6B.100d.txt') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(words_list, max_words=56):\n",
    "    output_reviews = []\n",
    "    for words in words_list:\n",
    "        words = [w.lower() for w in words]\n",
    "        words_mapped = [0]* max_words\n",
    "\n",
    "        length = len(words)\n",
    "        if (length<max_words):\n",
    "            #print('We should never see this print')\n",
    "            for i in range(0,length):\n",
    "                words_mapped[i] = embeddings_index.get(words[i], embeddings_index['unk'])\n",
    "            for i in range(length,max_words):\n",
    "                words_mapped[i] =  embeddings_index['unk']\n",
    "        elif (length>max_words):\n",
    "            print('We should never see this print either')\n",
    "        else:\n",
    "            for i in range(0,max_words):\n",
    "                words_mapped[i] = embeddings_index.get(words[i], embeddings_index['unk'])\n",
    "\n",
    "        output_reviews.append(words_mapped)\n",
    "\n",
    "    return output_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['embeddings'] = load_embeddings(train_data['token_list'])\n",
    "val_data['embeddings'] = load_embeddings(val_data['token_list'])\n",
    "test_data['embeddings'] = load_embeddings(test_data['token_list'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.to_pickle('./senti_data/train.p')\n",
    "val_data.to_pickle('./senti_data/val.p')\n",
    "test_data.to_pickle('./senti_data/test.p')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffle Labeled Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "att_data = pd.concat([train_data_with_att, val_data_with_att,test_data_with_att])\n",
    "att_data.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 1\n",
    "\n",
    "att_train_ix, att_test_ix = train_test_split(att_data.index, test_size=0.5, stratify=att_data['binary_label'], random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_without_att = pd.concat([train_data_without_att, val_data_without_att])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('senti_data/raw_text_with_att_train.npy', att_data.loc[att_train_ix]['token_list'])\n",
    "np.save('senti_data/raw_text_with_att_val_test.npy', att_data.loc[att_test_ix]['token_list'])\n",
    "np.save('senti_data/raw_text_without_att_train.npy', train_val_without_att['token_list'])\n",
    "np.save('senti_data/raw_text_without_att_val_test.npy', test_data_without_att['token_list'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('senti_data/y_with_att_train.npy', att_data.loc[att_train_ix]['binary_label'].astype(int))\n",
    "np.save('senti_data/y_with_att_val_test.npy', att_data.loc[att_test_ix]['binary_label'].astype(int))\n",
    "np.save('senti_data/y_without_att_train.npy', train_val_without_att['binary_label'].astype(int))\n",
    "np.save('senti_data/y_without_att_val_test.npy', test_data_without_att['binary_label'].astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('senti_data/x_with_att_train.npy', np.array(att_data.loc[att_train_ix]['embeddings'].tolist()))\n",
    "np.save('senti_data/x_with_att_val_test.npy', np.array(att_data.loc[att_test_ix]['embeddings'].tolist()))\n",
    "np.save('senti_data/x_without_att_train.npy', np.array(train_val_without_att['embeddings'].tolist()))\n",
    "np.save('senti_data/x_without_att_val_test.npy', np.array(test_data_without_att['embeddings'].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('senti_data/att_labels_with_att_train.npy', att_data.loc[att_train_ix]['attention'])\n",
    "np.save('senti_data/att_labels_with_att_val_test.npy', att_data.loc[att_test_ix]['attention'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "283px",
    "left": "1550px",
    "right": "20px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
